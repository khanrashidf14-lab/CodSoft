{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4e542a-9b3b-4eb2-ae28-a6d825b8995a",
   "metadata": {},
   "source": [
    "# Iris_Flower_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04138049-4fc8-4c8d-ab95-58119a9a0b50",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea71cfe-246b-49c2-b612-2e32a1496e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import sys\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# ---------------- SAMPLE TRAINING DATA ----------------\n",
    "SAMPLE_X = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],   # Setosa\n",
    "    [4.9, 3.0, 1.4, 0.2],   # Setosa\n",
    "    [7.0, 3.2, 4.7, 1.4],   # Versicolor\n",
    "    [6.4, 3.2, 4.5, 1.5],   # Versicolor\n",
    "    [6.3, 3.3, 6.0, 2.5],   # Virginica\n",
    "    [5.8, 2.7, 5.1, 1.9]    # Virginica\n",
    "])\n",
    "\n",
    "SAMPLE_y = np.array([\n",
    "    \"setosa\",\n",
    "    \"setosa\",\n",
    "    \"versicolor\",\n",
    "    \"versicolor\",\n",
    "    \"virginica\",\n",
    "    \"virginica\"\n",
    "])\n",
    "\n",
    "\n",
    "def build_pipeline(k: int | None = None) -> Tuple[Pipeline, dict]:\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "    if k is None:\n",
    "        param_grid = {\n",
    "            \"clf__n_neighbors\": list(range(1, 11)),\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "    else:\n",
    "        param_grid = {\n",
    "            \"clf__n_neighbors\": [k],\n",
    "            \"clf__weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "\n",
    "    return pipe, param_grid\n",
    "\n",
    "\n",
    "def train_and_evaluate(X: np.ndarray,\n",
    "                       y: np.ndarray,\n",
    "                       use_gridsearch: bool = True,\n",
    "                       param_grid: dict | None = None,\n",
    "                       cv: int = 5) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Train KNN pipeline on X,y. If use_gridsearch True, attempts GridSearchCV with cv,\n",
    "    but adjusts or disables GridSearch when there are too few samples per class.\n",
    "    Also automatically adjusts the n_neighbors search range so candidates are valid\n",
    "    for the (approximate) training fold size to avoid NaN scores.\n",
    "    \"\"\"\n",
    "    pipe, default_grid = build_pipeline()\n",
    "    grid_params = param_grid.copy() if param_grid is not None else default_grid.copy()\n",
    "\n",
    "    if not use_gridsearch:\n",
    "        pipe.fit(X, y)\n",
    "        return pipe\n",
    "\n",
    "    # to determine minimum number of samples per class\n",
    "    try:\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        min_count = int(counts.min())\n",
    "    except Exception:\n",
    "        min_count = 0\n",
    "\n",
    "    if min_count == 0:\n",
    "        warnings.warn(\"Could not determine class counts reliably; proceeding with GridSearchCV as requested.\")\n",
    "    else:\n",
    "        if cv > min_count:\n",
    "            warnings.warn(\n",
    "                f\"Requested cv={cv} is greater than the smallest class size ({min_count}). \"\n",
    "                f\"Reducing cv to {min_count} for GridSearchCV.\"\n",
    "            )\n",
    "            cv = min_count\n",
    "\n",
    "    # Estimate training set size per fold and restrict n_neighbors accordingly\n",
    "    n_samples = X.shape[0]\n",
    "    # approximate train size per fold = (cv-1)/cv * n_samples\n",
    "    n_train_est = max(1, int((cv - 1) / cv * n_samples))\n",
    "    max_k_allowed = max(1, min(10, n_train_est))  # cap at 10 by default, but don't exceed estimated train size\n",
    "\n",
    "    if \"clf__n_neighbors\" in grid_params:\n",
    "        original_range = grid_params[\"clf__n_neighbors\"]\n",
    "        try:\n",
    "            grid_params[\"clf__n_neighbors\"] = list(range(1, max_k_allowed + 1))\n",
    "            print(f\"Adjusted n_neighbors search range to 1..{max_k_allowed} (estimated train size per fold = {n_train_est}).\")\n",
    "        except Exception:\n",
    "            grid_params[\"clf__n_neighbors\"] = original_range\n",
    "\n",
    "    if cv < 2:\n",
    "        warnings.warn(\n",
    "            \"Not enough members per class to perform cross-validation. \"\n",
    "            \"Skipping GridSearchCV and fitting the pipeline directly.\"\n",
    "        )\n",
    "        pipe.fit(X, y)\n",
    "        return pipe\n",
    "\n",
    "    #GridSearchCV with adjusted cv and adjusted n_neighbors grid\n",
    "    grid = GridSearchCV(pipe, grid_params, cv=cv, n_jobs=-1, verbose=0)\n",
    "    grid.fit(X, y)\n",
    "    best = grid.best_estimator_\n",
    "    print(f\"\\nBest parameters from GridSearchCV: {grid.best_params_}\")\n",
    "    #cross-validated score for the best estimator\n",
    "    cv_scores = cross_val_score(best, X, y, cv=cv, n_jobs=-1)\n",
    "    print(f\"Cross-validated accuracy (mean ± std): {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    return best\n",
    "\n",
    "\n",
    "def show_evaluation(model: Pipeline, X_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nTest accuracy: {acc:.3f}\\n\")\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "\n",
    "def parse_input_sample(s: str) -> np.ndarray | None:\n",
    "    s = s.strip()\n",
    "    if s.lower() in (\"q\", \"quit\", \"exit\"):\n",
    "        return None\n",
    "    parts = [p.strip() for p in s.split(\",\")]\n",
    "    if len(parts) != 4:\n",
    "        raise ValueError(\"Expected 4 comma-separated numbers (sepal_length,sepal_width,petal_length,petal_width).\")\n",
    "    try:\n",
    "        floats = [float(p) for p in parts]\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\"All 4 values must be numeric.\") from e\n",
    "    return np.array([floats])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d002c2-bc0f-4db7-9076-482c442129f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN model...\n",
      "Adjusted n_neighbors search range to 1..3 (estimated train size per fold = 3).\n",
      "\n",
      "Best parameters from GridSearchCV: {'clf__n_neighbors': 2, 'clf__weights': 'uniform'}\n",
      "Cross-validated accuracy (mean ± std): 0.667 ± 0.000\n",
      "\n",
      "Enter samples as: sepal_length,sepal_width,petal_length,petal_width\n",
      "Or type 'q' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABDUL WAKEEL\\AppData\\Local\\Temp\\ipykernel_11924\\1304206782.py:85: UserWarning: Requested cv=5 is greater than the smallest class size (2). Reducing cv to 2 for GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sample>  2.6,5.6,2.4,2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted species: setosa\n",
      "Class probabilities:\n",
      "  setosa      : 0.500\n",
      "  virginica   : 0.500\n",
      "  versicolor  : 0.000\n",
      "\n",
      "2 nearest neighbors (distance, class):\n",
      "  idx=  0  dist=9.410  label=0\n",
      "  idx=  4  dist=10.670  label=2\n",
      "\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sample>  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting interactive loop.\n"
     ]
    }
   ],
   "source": [
    "def interactive_predict_loop(model: Pipeline) -> None:\n",
    "    clf: KNeighborsClassifier = model.named_steps[\"clf\"]\n",
    "    scaler: StandardScaler = model.named_steps[\"scaler\"]\n",
    "    classes = clf.classes_\n",
    "\n",
    "    print(\"\\nEnter samples as: sepal_length,sepal_width,petal_length,petal_width\")\n",
    "    print(\"Or type 'q' to quit.\\n\")\n",
    "    try:\n",
    "        while True:\n",
    "            line = input(\"Sample> \").strip()\n",
    "            if line.lower() in (\"q\", \"quit\", \"exit\"):\n",
    "                print(\"Exiting interactive loop.\")\n",
    "                break\n",
    "            try:\n",
    "                X = parse_input_sample(line)\n",
    "                if X is None:\n",
    "                    break\n",
    "            except ValueError as e:\n",
    "                print(\"Invalid input:\", e)\n",
    "                continue\n",
    "\n",
    "            X_scaled = scaler.transform(X)\n",
    "            pred = clf.predict(X_scaled)[0]\n",
    "            probs = clf.predict_proba(X_scaled)[0]\n",
    "\n",
    "            prob_pairs = sorted(zip(classes, probs), key=lambda t: t[1], reverse=True)\n",
    "            print(f\"\\nPredicted species: {pred}\")\n",
    "            print(\"Class probabilities:\")\n",
    "            for label, p in prob_pairs:\n",
    "                print(f\"  {label:12s}: {p:.3f}\")\n",
    "\n",
    "            k_used = clf.n_neighbors\n",
    "            distances, indices = clf.kneighbors(X_scaled, n_neighbors=k_used)\n",
    "            print(f\"\\n{k_used} nearest neighbors (distance, class):\")\n",
    "            neighbor_label = getattr(clf, \"y\", None) or getattr(clf, \"_y\", None)\n",
    "            for d, idx in zip(distances[0], indices[0]):\n",
    "                label_text = \"unknown\"\n",
    "                if neighbor_label is not None:\n",
    "                    try:\n",
    "                        label_text = neighbor_label[idx]\n",
    "                    except Exception:\n",
    "                        label_text = \"unknown\"\n",
    "                print(f\"  idx={idx:3d}  dist={d:.3f}  label={label_text}\")\n",
    "            print(\"\\n\" + \"-\" * 40)\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"\\nInteractive session terminated.\")\n",
    "\n",
    "\n",
    "def main(argv: List[str] | None = None) -> None:\n",
    "    parser = argparse.ArgumentParser(description=\"Advanced KNN Iris classifier (interactive).\")\n",
    "    parser.add_argument(\"--full\", action=\"store_true\", help=\"Use the full sklearn iris dataset (otherwise uses small sample).\")\n",
    "    parser.add_argument(\"--k\", type=int, default=None, help=\"Fix k (n_neighbors) to this value instead of searching.\")\n",
    "    parser.add_argument(\"--cv\", type=int, default=5, help=\"Number of CV folds for GridSearch/CV.\")\n",
    "    parser.add_argument(\"--no-search\", action=\"store_true\", help=\"Don't run GridSearch; train with provided/ default params.\")\n",
    "    parser.add_argument(\"--save\", type=str, default=None, help=\"Path to save the trained model (joblib).\")\n",
    "\n",
    "    # When running in notebooks, allow kernel args by using parse_known_args()\n",
    "    if argv is None:\n",
    "        args, _unknown = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args(argv)\n",
    "\n",
    "    if args.full:\n",
    "        iris = load_iris()\n",
    "        X = iris.data\n",
    "        y = iris.target_names[iris.target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "        use_eval = True\n",
    "    else:\n",
    "        X_train, y_train = SAMPLE_X, SAMPLE_y\n",
    "        X_test, y_test = None, None\n",
    "        use_eval = False\n",
    "\n",
    "    print(\"Training KNN model...\")\n",
    "    pipe, default_grid = build_pipeline(k=args.k)\n",
    "    if args.no_search:\n",
    "        final_model = train_and_evaluate(X_train, y_train, use_gridsearch=False)\n",
    "    else:\n",
    "        final_model = train_and_evaluate(X_train, y_train, use_gridsearch=True, param_grid=default_grid, cv=args.cv)\n",
    "\n",
    "    if use_eval and X_test is not None:\n",
    "        show_evaluation(final_model, X_test, y_test)\n",
    "\n",
    "    interactive_predict_loop(final_model)\n",
    "\n",
    "    if args.save:\n",
    "        try:\n",
    "            joblib.dump(final_model, args.save)\n",
    "            print(f\"Model saved to {args.save}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save model: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7375e-b3c0-4e5a-90ac-0ba17b1e17c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
